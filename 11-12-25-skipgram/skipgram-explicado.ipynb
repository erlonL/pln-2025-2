{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f16f64",
   "metadata": {},
   "source": [
    "## Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init len e getitem são metodos especiais e sempre precisam existir\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = []\n",
    "\n",
    "        for row in dataset:\n",
    "            self.data.extend(row['window'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = Word2VecDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67481fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(my_dataset, batch_size=1024, shuffle=True) \n",
    "# shuffle: ordem pode enviesar o treinamento\n",
    "# batch_size: quanto mais exemplos passamos garante com que o treino seja menos caótico,\n",
    "#  porque o erro vai ficar mais \"smooth\"\n",
    "#  ao invés de atualizar a cada exemplo, atualizamos por lote\n",
    "#  batch muito grande pode fazer estourar a memória da gpu (com datasets grandes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4235fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # camadas da nn\n",
    "        #   camada de embedding\n",
    "        #   vocab: tamanho da entrada, mapea cada token único\n",
    "        #   embedding: tamanho de saída\n",
    "        #   tensor: array multidimensional necessário do pytorch. pra cada índice que eu tenho,\n",
    "        #           retorna um vetor de tamanho = embedding_dim\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        #   fully connected\n",
    "        #   entrada: tamanho do embedding\n",
    "        #   saída: tamanho do vocabulário ([0.007, 0.003, 0.96, ..., 0.001, 0.0001, 0.001]),\n",
    "        #        pegamos qual a palavra por ID / posição\n",
    "        #        sempre normalizamos pra soma do vetor dar 1\n",
    "        self.expand = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # só precisamos fazer o forward, o backpropagation é gerenciado pela biblioteca\n",
    "        embed_vector = self.embed(x)\n",
    "        output = self.expand(embed_vector)\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3001652",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_test = nn.Embedding(len(vocab), 50)\n",
    "\n",
    "display(embedding_test(torch.tensor(10))) # retorna um vetor de tamanho 50 inicializado de forma aleatória\n",
    "\n",
    "display(embedding_test(torch.tensor(10)).shape) # 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Word2Vec(len(vocab), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041843a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "network(torch.tensor(10)) # chama o forward\n",
    "\n",
    "display(network(torch.tensor(10)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = network(torch.tensor(10))\n",
    "\n",
    "display(result.argmax())\n",
    "\n",
    "result[30983]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236be4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token[10], id2token[30983]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38336f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ultimo passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "EPOCHS = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# se eu tenho valores que são iguais vai tender a 0. Quanto mais dif tende a infinito\n",
    "optimizer = torch.optim.AdamW(network.parameters(), lr=LR)\n",
    "# otimizador: baseados no gradiente descendente, mas tem alguns ajustes de LR de forma dinâmica\n",
    "#   pra garantir aprendizado\n",
    "#   no começo ajustamos bastante, talvez no final precisemos ajustar bem pouco\n",
    "#   outro uso: você usou um tamanho de passo, se for bom aumenta, se começar a piorar diminui\n",
    "\n",
    "# o dado e a rede precisa estar no mesmo device\n",
    "device = torch.device('cuda')\n",
    "network.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for input, context in dataloader:\n",
    "        input = input.to(device)\n",
    "        print('input: ', input)\n",
    "        print('input shape: ', input.shape)\n",
    "        context = context.to(device)\n",
    "        print('context: ', context)\n",
    "        print('context shape: ', context.shape)\n",
    "\n",
    "        # input: primeiro elemento da tupla\n",
    "        # context: segundo elemento da tupla\n",
    "        # ('house', 'dog')\n",
    "\n",
    "\n",
    "        output = network(input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # fazemos pra fazer uma nova atualização de peso\n",
    "\n",
    "        loss = loss_fn(output, context) # saber quanto erramoszes\n",
    "        # debug\n",
    "        break\n",
    "\n",
    "        epoch_loss += loss.item() # monitorar as losszes\n",
    "\n",
    "        loss.backward() # calcula os gradientes\n",
    "        optimizer.step() # atualiza os pesos\n",
    "\n",
    "    # torch.save(model.state_dict(), f'word2vec-{i}.pt')\n",
    "    all_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {i} loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b06e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for input, context in tqdm(dataloader):\n",
    "        input = input.to(device)\n",
    "        context = context.to(device)\n",
    "\n",
    "        output = network(input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # fazemos pra fazer uma nova atualização de peso\n",
    "\n",
    "        loss = loss_fn(output, context) # saber quanto erramoszes\n",
    "        epoch_loss += loss.item() # monitorar as losszes\n",
    "\n",
    "        loss.backward() # calcula os gradientes\n",
    "        optimizer.step() # atualiza os pesos\n",
    "\n",
    "    # torch.save(model.state_dict(), f'word2vec-{i}.pt')\n",
    "    all_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {i} loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c60c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network.state_dict(), 'word2vec.pt')\n",
    "torch.load('word2vec.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# como acessar os embeddings?\n",
    "\n",
    "word2vecs = network.expand.weight.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c33397",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecs[token2id['here']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distancia das palavras\n",
    "\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = word2vecs[token2id['father']]\n",
    "w2 = word2vecs[token2id['mother']]\n",
    "\n",
    "distance.pdist([w1, w2], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = word2vecs[token2id['mother']]\n",
    "w2 = word2vecs[token2id['hate']]\n",
    "\n",
    "distance.pdist([w1, w2], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413be520",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = word2vecs[token2id['hate']]\n",
    "w2 = word2vecs[token2id['speech']]\n",
    "\n",
    "distance.pdist([w1, w2], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c40d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = word2vecs[token2id['mars']]\n",
    "w2 = word2vecs[token2id['milk']]\n",
    "\n",
    "distance.pdist([w1, w2], 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular rankeamento de palavras mais similares\n",
    "# pode ser feito com sklearn cosine similarity\n",
    "\n",
    "word_embedding = word2vecs[token2id['hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualização dos embeddings, transformando os vetores em 2 dimensões    \n",
    "# TAREFINHA DE CASAAAAAAAAAAAA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".localvenv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
